{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d3/ltp3h01s1lxfh66qwpy5nv5w0000gn/T/ipykernel_23713/328227645.py:1: DtypeWarning: Columns (107,109,110,114,116,117,121,123,124,129,132,133) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"sampled_x_train_t.csv\")  # Randomized sample of the big dataset\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sampled_x_train_t.csv\")  # Randomized sample of the big dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning all redundant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(\n",
    "    df: pd.DataFrame,\n",
    "    piezo: bool = False,\n",
    "    meteo: bool = False,\n",
    "    hydro: bool = False,\n",
    "    prelev: bool = False,\n",
    "    insee: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    # Define columns to exclude for each category\n",
    "    excluded_piezo = [\n",
    "        \"piezo_station_department_code\",\n",
    "        \"piezo_station_update_date\",\n",
    "        \"piezo_station_department_name\",\n",
    "        \"piezo_station_commune_code_insee\",\n",
    "        \"piezo_station_pe_label\",\n",
    "        \"piezo_station_bdlisa_codes\",\n",
    "        \"piezo_station_bss_code\",\n",
    "        \"piezo_station_commune_name\",\n",
    "        \"piezo_station_bss_id\",\n",
    "        \"piezo_bss_code\",\n",
    "        \"piezo_continuity_name\",\n",
    "        \"piezo_producer_code\",\n",
    "        \"piezo_producer_name\",\n",
    "        \"piezo_measure_nature_name\",\n",
    "    ]\n",
    "    excluded_meteo = [\"meteo_id\", \"meteo_radiation_IR\", \"meteo_name\"]\n",
    "    excluded_hydro = [\n",
    "        \"hydro_station_code\",\n",
    "        \"hydro_status_label\",\n",
    "        \"hydro_method_code\",\n",
    "        \"hydro_method_label\",\n",
    "        \"hydro_qualification_label\",\n",
    "    ]\n",
    "    excluded_prelev = [\n",
    "        \"prelev_structure_code_0\",\n",
    "        \"prelev_volume_0\",\n",
    "        \"prelev_usage_label_0\",\n",
    "        \"prelev_volume_obtention_mode_label_0\",\n",
    "        \"prelev_longitude_0\",\n",
    "        \"prelev_latitude_0\",\n",
    "        \"prelev_commune_code_insee_0\",\n",
    "        \"prelev_structure_code_1\",\n",
    "        \"prelev_volume_1\",\n",
    "        \"prelev_usage_label_1\",\n",
    "        \"prelev_volume_obtention_mode_label_1\",\n",
    "        \"prelev_longitude_1\",\n",
    "        \"prelev_latitude_1\",\n",
    "        \"prelev_commune_code_insee_1\",\n",
    "        \"prelev_structure_code_2\",\n",
    "        \"prelev_volume_2\",\n",
    "        \"prelev_usage_label_2\",\n",
    "        \"prelev_volume_obtention_mode_label_2\",\n",
    "        \"prelev_longitude_2\",\n",
    "        \"prelev_latitude_2\",\n",
    "        \"prelev_commune_code_insee_2\",\n",
    "        \"prelev_other_volume_sum\",\n",
    "    ]\n",
    "    excluded_insee = [\n",
    "        \"prelev_commune_code_insee_0\",\n",
    "        \"prelev_commune_code_insee_1\",\n",
    "        \"prelev_commune_code_insee_2\",\n",
    "        \"insee_%_agri\",\n",
    "        \"insee_pop_commune\",\n",
    "        \"insee_med_living_level\",\n",
    "        \"insee_%_ind\",\n",
    "        \"insee_%_const\",\n",
    "    ]\n",
    "    # Combine columns to drop based on the parameters\n",
    "    columns_to_drop = []\n",
    "    if piezo:\n",
    "        columns_to_drop += excluded_piezo\n",
    "    if meteo:\n",
    "        columns_to_drop += excluded_meteo\n",
    "    if hydro:\n",
    "        columns_to_drop += excluded_hydro\n",
    "    if prelev:\n",
    "        columns_to_drop += excluded_prelev\n",
    "    if insee:\n",
    "        columns_to_drop += excluded_insee\n",
    "\n",
    "    # Drop columns safely (ignore errors for missing columns)\n",
    "\n",
    "    df_filtered = df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifies longitude and latitude columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_long_lat(df: pd.DataFrame, distance_threshold: int = 25) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Unifies longitude and latitude into single columns if distance_piezo_hydro is below the threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        distance_threshold (int): The maximum distance for unification.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with unified longitude and latitude columns.\n",
    "    \"\"\"\n",
    "    # Columns to exclude after processing\n",
    "    excluding = [\n",
    "        \"piezo_station_latitude\",\n",
    "        \"piezo_station_longitude\",\n",
    "        \"hydro_longitude\",\n",
    "        \"hydro_latitude\",\n",
    "        \"meteo_longitude\",\n",
    "        \"meteo_latitude\",\n",
    "        \"distance_piezo_hydro\",\n",
    "        \"distance_hydro_meteo\",\n",
    "    ]\n",
    "\n",
    "    # Ensure the required column exists\n",
    "    if \"distance_piezo_hydro\" in df.columns:\n",
    "        # Create unified longitude and latitude where condition is met\n",
    "        df.loc[df[\"distance_piezo_hydro\"] < distance_threshold, \"longitude\"] = df[\n",
    "            \"piezo_station_longitude\"\n",
    "        ]\n",
    "        df.loc[df[\"distance_piezo_hydro\"] < distance_threshold, \"latitude\"] = df[\n",
    "            \"piezo_station_latitude\"\n",
    "        ]\n",
    "\n",
    "    # Drop the excluded columns\n",
    "    df = df.drop(columns=excluding, errors=\"ignore\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Unifies date combine date in a single column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with unifies dates columns.\n",
    "    \"\"\"\n",
    "    # Columns to exclude after processing\n",
    "    excluding = [\"piezo_measurement_date\", \"meteo_date\", \"hydro_observation_date_elab\"]\n",
    "\n",
    "    # Check and assign the first available date column\n",
    "    if \"piezo_measurement_date\" in df.columns:\n",
    "        df[\"date\"] = df[\"piezo_measurement_date\"]\n",
    "\n",
    "    df = df.drop(columns=excluding, errors=\"ignore\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning residual empty value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_essential_data(df: pd.DataFrame, threshold: float = 0.8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keeping essential data by removing columns with less than thrshold % of empty data\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "        threshold (float): % of empty data in a column\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute threshold % not null value for each columns\n",
    "    non_null_percentage = df.notnull().mean()  # not null values %\n",
    "\n",
    "    # Select only column with less 80 % not null values\n",
    "    columns_to_keep = non_null_percentage[non_null_percentage >= threshold].index\n",
    "    filtered_df = df[columns_to_keep]\n",
    "\n",
    "    # Print datasat after filtration\n",
    "    print(\"Initals columns :\", df.columns.tolist())\n",
    "    print(\"Keeped columns :\", filtered_df.columns.tolist())\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning piezzo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piezzo_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the DataFrame by applying a series of cleaning and transformation steps for piezzo data.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace NaN in qualification columns\n",
    "    df[\"piezo_qualification\"] = df[\"piezo_qualification\"].fillna(\"Incertaine\")\n",
    "\n",
    "    # Replace NaN in piezo_measure_nature_code\n",
    "    df[\"piezo_measure_nature_code\"] = df[\"piezo_measure_nature_code\"].fillna(0)\n",
    "\n",
    "    # Replace NaN in piezo_obtention_mode\n",
    "    df[\"piezo_obtention_mode\"] = df[\"piezo_obtention_mode\"].fillna(\n",
    "        \"Mode d'obtention inconnu\"\n",
    "    )\n",
    "\n",
    "    # Replace NaN in piezo_status\n",
    "    df[\"piezo_status\"] = df[\"piezo_status\"].fillna(\"Donn√©e brute\")\n",
    "\n",
    "    # Define a mapping for the labels to integers\n",
    "    qualification_label_mapping = {\n",
    "        \"Correcte\": 3,\n",
    "        \"Non qualifi√©\": 2,\n",
    "        \"Incorrecte\": 0,\n",
    "        \"Incertaine\": 1,\n",
    "    }\n",
    "\n",
    "    # Map the labels to integers\n",
    "    df[\"piezo_qualification\"] = df[\"piezo_qualification\"].map(\n",
    "        qualification_label_mapping\n",
    "    )\n",
    "\n",
    "    # Mapping obtention mode\n",
    "    elements = [\"piezo_status\", \"piezo_obtention_mode\"]\n",
    "    df = pd.get_dummies(df, columns=elements, drop_first=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling others NaN values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss_imputer import FaissImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def impute_na(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute missing values in a DataFrame using FaissImputer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame with missing values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Drop non-numerical columns\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\", \"datetime64\"]).columns\n",
    "    df = df.drop(columns=categorical_cols)\n",
    "\n",
    "    # Create an instance of FaissImputer\n",
    "    imputer = FaissImputer(n_neighbors=10)\n",
    "    simple_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    # Fit the imputer on the data frame with missing values\n",
    "    imputer.fit(df)\n",
    "\n",
    "    # Transform the data frame with missing values\n",
    "    df = pd.DataFrame(imputer.transform(df), columns=df.columns)\n",
    "    df = pd.DataFrame(simple_imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def date_to_timestamp_convertion(df: pd.DataFrame):\n",
    "    # Convert the date column to datetime and then to a timestamp\n",
    "    df[\"date\"] = (\n",
    "        pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\").astype(int)\n",
    "        // 10**9\n",
    "    )\n",
    "\n",
    "    # Normalize for better scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    df[\"date\"] = scaler.fit_transform(df[[\"date\"]])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_na(df):\n",
    "    # Identify string/object columns\n",
    "    string_features = list(df.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "    # Exclude specific columns (like 'piezo_groundwater_level_category')\n",
    "    string_features.remove(\"piezo_groundwater_level_category\")\n",
    "\n",
    "    # Initialize the LabelEncoder\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    # Apply encoding with handling for mixed types and missing values\n",
    "    df[string_features] = df[string_features].apply(\n",
    "        lambda col: le.fit_transform(col.astype(str).fillna(\"Unknown\"))\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the DataFrame by applying a series of cleaning and transformation steps.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        cleaning_params (dict): Dictionary of parameters for the `cleaning` function.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    df = cleaning(df, True, True, True, True, True)\n",
    "    df = unify_long_lat(df)\n",
    "    df = unify_date(df)\n",
    "    df = date_to_timestamp_convertion(df)\n",
    "    df = piezzo_clean(df)\n",
    "    df = drop_na(df)\n",
    "    # df = impute_na(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pre_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for the labels to integers\n",
    "label_mapping = {\"Very Low\": 0, \"Low\": 1, \"Average\": 2, \"High\": 3, \"Very High\": 4}\n",
    "\n",
    "# Map the labels to integers\n",
    "df[\"piezo_groundwater_level_category\"] = df[\"piezo_groundwater_level_category\"].map(\n",
    "    label_mapping\n",
    ")\n",
    "\n",
    "df.to_csv(\"pre-processed_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
